{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === STEP 1: SETUP ===\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (You will be asked to click a link and authorize)\n",
        "print(\"üìÇ Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install AI Libraries\n",
        "print(\"‚è≥ Installing libraries...\")\n",
        "os.system('pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet')\n",
        "os.system('pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes --quiet')\n",
        "os.system('pip install langchain langchain-community langchain-huggingface chromadb flask pyngrok duckduckgo-search --quiet')\n",
        "\n",
        "print(\"‚úÖ Step 1 Complete: Drive mounted and libraries installed.\")\n"
      ],
      "metadata": {
        "id": "RkzUy0Vm9-84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 2: SMART DATASET LOADING ===\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Define the path in Google Drive\n",
        "dataset_path = \"/content/drive/My Drive/my_training_data.jsonl\"\n",
        "\n",
        "# 2. Check if you already have a custom dataset\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"üìÇ Found custom dataset at: {dataset_path}\")\n",
        "    print(\"   Using your personal data for training.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No custom dataset found. Creating a default one for you...\")\n",
        "\n",
        "    # --- DEFAULT DATA (Generic Template) ---\n",
        "    default_data = [\n",
        "        {\"instruction\": \"Who are you?\", \"output\": \"I am an AI assistant trained to help you.\"},\n",
        "        {\"instruction\": \"What can you do?\", \"output\": \"I can answer questions, write code, and assist with tasks.\"},\n",
        "        {\"instruction\": \"Explain AI.\", \"output\": \"AI stands for Artificial Intelligence, which enables computers to mimic human logic.\"},\n",
        "        {\"instruction\": \"Write a hello world in Python.\", \"output\": \"print('Hello World')\"}\n",
        "    ]\n",
        "\n",
        "    # Save this default data to Drive so you can edit it later\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        for entry in default_data:\n",
        "            json.dump(entry, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"‚úÖ Created default dataset at: {dataset_path}\")\n",
        "    print(\"   (To use your own data, just edit this file in your Google Drive!)\")\n",
        "\n",
        "print(\"‚úÖ Step 2 Complete: Dataset is ready.\")"
      ],
      "metadata": {
        "id": "PspDwIfv-BMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 3: TRAIN MODEL ===\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 1. Load Base Model\n",
        "print(\"‚è≥ Loading Llama-3.1-8B Model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# 2. Add LoRA Adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 3. Define Format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_func(examples):\n",
        "    texts = []\n",
        "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        texts.append(alpaca_prompt.format(instruction, output) + tokenizer.eos_token)\n",
        "    return texts\n",
        "\n",
        "# 4. Load Dataset (FROM GOOGLE DRIVE NOW)\n",
        "dataset_file = \"/content/drive/My Drive/my_training_data.jsonl\"\n",
        "print(f\"üìÇ Loading dataset from: {dataset_file}\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "# 5. Start Training\n",
        "print(\"üöÄ Starting Fine-Tuning...\")\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 1,\n",
        "    formatting_func = formatting_func,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 6. Save Locally (Temporary)\n",
        "print(\"üíæ Saving temporary model...\")\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"‚úÖ Step 3 Complete: Training finished.\")"
      ],
      "metadata": {
        "id": "dXep5AgSApKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 4: CREATE KNOWLEDGE BASE (RAG) ===\n",
        "import os\n",
        "\n",
        "# 1. Define the path in Google Drive\n",
        "rag_path = \"/content/drive/My Drive/my_knowledge.txt\"\n",
        "\n",
        "# 2. Check if it already exists\n",
        "if os.path.exists(rag_path):\n",
        "    print(f\"üìÇ Found existing knowledge base at: {rag_path}\")\n",
        "    print(\"   (The bot will read facts from this file)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Creating a new knowledge base...\")\n",
        "\n",
        "    # --- DEFAULT FACTS (The \"Cheat Sheet\") ---\n",
        "    # You can edit this file in your Google Drive anytime!\n",
        "    knowledge_text = \"\"\"\n",
        "Identity:\n",
        "- Name: CrownClown\n",
        "- Creator: Joshua\n",
        "- Creation Year: 2026\n",
        "- Architecture: Llama 3.1 8B (Fine-tuned)\n",
        "\n",
        "User Preferences:\n",
        "- The user is a developer.\n",
        "- Prefers code examples over long explanations.\n",
        "\"\"\"\n",
        "\n",
        "    with open(rag_path, \"w\") as f:\n",
        "        f.write(knowledge_text)\n",
        "\n",
        "    print(f\"‚úÖ Created default knowledge file at: {rag_path}\")\n",
        "\n",
        "print(\"‚úÖ Step 4 Complete: Knowledge Base is ready.\")"
      ],
      "metadata": {
        "id": "v6i0LIAEApzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 5: PERMANENT SAVE TO DRIVE ===\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "source_folder = \"lora_model\"\n",
        "destination_folder = \"/content/drive/My Drive/CrownClown_Model\"\n",
        "\n",
        "print(f\"üíæ Copying '{source_folder}' to Google Drive...\")\n",
        "\n",
        "# 1. Clean up old version in Drive (if exists)\n",
        "if os.path.exists(destination_folder):\n",
        "    shutil.rmtree(destination_folder)\n",
        "\n",
        "# 2. Copy the new model\n",
        "shutil.copytree(source_folder, destination_folder)\n",
        "\n",
        "print(f\"‚úÖ Step 5 Complete: Model successfully saved to {destination_folder}\")"
      ],
      "metadata": {
        "id": "dmxknjcHDULO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 5: FINAL SERVER (WITH MEMORY) ===\n",
        "# üî¥ PASTE YOUR NGROK TOKEN BELOW üî¥\n",
        "NGROK_AUTH_TOKEN = \"387ajRwhcZVYCEMbS8PYwjrR04v_86Ws1H64EPTQx7c5bM25F\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok, conf\n",
        "from unsloth import FastLanguageModel\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "# 1. Load Model\n",
        "drive_model_path = \"/content/drive/My Drive/CrownClown_Model\"\n",
        "print(f\"‚è≥ Loading CrownClown from: {drive_model_path}...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = drive_model_path,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 2. Setup RAG\n",
        "if not os.path.exists(\"my_knowledge.txt\"):\n",
        "    with open(\"my_knowledge.txt\", \"w\") as f: f.write(\"Identity: CrownClown AI.\")\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_rag_context(query):\n",
        "    if not os.path.exists(\"my_knowledge.txt\"): return \"\"\n",
        "    with open(\"my_knowledge.txt\", \"r\") as f: doc_text = f.read()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    docs = [Document(page_content=x) for x in text_splitter.split_text(doc_text)]\n",
        "    if not docs: return \"\"\n",
        "    db = Chroma.from_documents(docs, embedding_model)\n",
        "    results = db.similarity_search(query, k=1)\n",
        "    return results[0].page_content if results else \"\"\n",
        "\n",
        "def search_web(query):\n",
        "    try:\n",
        "        results = DDGS().text(query, max_results=3)\n",
        "        return \"\\n\".join([f\"- {r['title']}: {r['body']}\" for r in results]) if results else \"\"\n",
        "    except: return \"\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    user_message = data.get(\"message\", \"\")\n",
        "    # FRONTEND sends the history now!\n",
        "    chat_history = data.get(\"history\", [])\n",
        "    use_web = data.get(\"use_web\", False)\n",
        "\n",
        "    # 1. Get Context\n",
        "    rag_context = get_rag_context(user_message)\n",
        "    web_context = \"\"\n",
        "    if use_web or any(k in user_message.lower() for k in [\"plan\", \"price\", \"news\", \"trip\", \"weather\"]):\n",
        "        web_context = search_web(user_message)\n",
        "\n",
        "    # 2. Format History from the Frontend\n",
        "    # (The frontend sends [\"User: Hi\", \"Bot: Hello\"], we join it)\n",
        "    history_text = \"\\n\".join(chat_history[-6:])\n",
        "\n",
        "    # 3. Construct Prompt with History\n",
        "    # We tell it: \"Here is what we said before. Continue the conversation.\"\n",
        "    input_text = f\"\"\"You are CrownClown.\n",
        "Context: {rag_context}\n",
        "Web Info: {web_context}\n",
        "\n",
        "PREVIOUS CHAT:\n",
        "{history_text}\n",
        "\n",
        "CURRENT USER QUESTION: {user_message}\n",
        "System Note: Do not introduce yourself if you already did in the previous chat. Answer directly.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        use_cache=True,\n",
        "        stop_strings=[\"### Instruction\"],\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    final_answer = response.split(\"### Response:\")[-1].split(\"### Instruction\")[0].replace(\"<|eot_id|>\", \"\").strip()\n",
        "    is_code = \"```\" in final_answer\n",
        "\n",
        "    # No longer saving to internal memory (HISTORY) as it's stateless\n",
        "\n",
        "    return jsonify({\"response\": final_answer, \"is_code\": is_code})\n",
        "\n",
        "# Run\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f\"\\nüöÄ SMART SERVER LIVE: {public_url}\")\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "nUNbDwyqB_CR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}